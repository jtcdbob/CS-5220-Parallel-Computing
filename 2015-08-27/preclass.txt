## Reading questions

1.  A given program spends 10% of its time in an initial startup
    phase, and then 90% of its time in work that can be easily
    parallelized.  Assuming a machine with homogeneous cores, plot the
    idealized speedup and parallel efficiency of the overall code
    according to Amdahl's law for up to 128 cores.  If you know how,
    you should use a script to produce this plot, with both the serial
    fraction and the maximum number of cores as parameters.
A:  According to Amdahl's law, the time a parallel code consumes is (assuming n cores):
        T(n) = T(1)(P + (1-P)/n)
        where T(n) is the time for parallel code and T(1) is the time for non-parallel code, P is the portion of code that can be parallelized. So the speedup efficiency is:
        R(n) = P + (1-P)/n
2.  Suppose a particular program can be partitioned into perfectly
    independent tasks, each of which takes time tau.  Tasks are
    set up, scheduled, and communicated to p workers at a (serial)
    central server; this takes an overhead time alpha per task.
    What is the theoretically achievable throughput (tasks/time)?
A:  For each task, it takes alpha overhead time, so for n tasks, the total time needed for the job is
        T = n*alpha + tau
    The time for each task will be:
        t = T/n = alpha + tau/n
    The throughput is then
        1/t = 1/(alpha + tau/n) -> 1/alpha  as n -> infinity
    So the theoretically achievable throughput is 1/alpha.
3.  Under what circumstances is it best to not tune?
    It is best not to tune if it is not needed or not worth the human labor to do so. Tuning should be considered if at least the code is:
        a. going to be used repeatedly and a reduction in running time will be highly beneficial.
        b. optimized in the sense that a better algorithm has been employed and a parallelization is indispensable to reduce the running time of key routines.
        c. heavily bottlenecked by routines that is best tackled by parallel computing.
    Because it is expensive to implement and maintain a fine-tuned code, it is advisable to not tune if other approaches have not been fully exploited.
4.  The class cluster consists of eight nodes and fifteen Xeon Phi
    accelerator boards.  Based on an online search for information on
    these systems, what do you think is the theoretical peak flop rate
    (double-precision floating point operations per second)?  Show how
    you computed this, and give URLs for where you got the parameters
    in your calculation.  (We will return to this question again after
    we cover some computer architecture.)
    From what I found online:
    http://ark.intel.com/products/71992/Intel-Xeon-Phi-Coprocessor-5110P-8GB-1_053-GHz-60-core
    http://stackoverflow.com/questions/15655835/flops-per-cycle-for-sandy-bridge-and-haswell-sse2-avx-avx2
    http://scicomp.stackexchange.com/questions/11306/how-to-determine-the-amount-of-flops-my-computer-is-capable-of
        The Intel Xeon Phi 5110P has base frequency of 1.053 GHz and it does 16 flops per cycle. So the theoretical peak flop rate is
        1.053 GHz * 16 Flops/cycle * 60 cores * 8 nodes = 8.09 Tflops/s
5.  What is the approximate theoretical peak flop rate for your own machine?
    My machine has Intel Core i5-4258U Processor with specs that can be found in
    http://ark.intel.com/products/75990/Intel-Core-i5-4258U-Processor-3M-Cache-up-to-2_90-GHz
    It has peak frequency of 2.9 GHz and 2 cores. Since it's a Haswell, according to one of the links in question 4, it can do 16 flops/cycle. The peak flop rate is
        2.9 GHz * 16 flops/cycle * 2 cores = 92.8 Gflops/s
